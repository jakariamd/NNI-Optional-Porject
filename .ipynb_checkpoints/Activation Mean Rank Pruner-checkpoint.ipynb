{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559a9102",
   "metadata": {},
   "source": [
    "# Load Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63519f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from nni.compression.pytorch.speedup import ModelSpeedup\n",
    "from nni.compression.pytorch.utils import count_flops_params\n",
    "import time\n",
    "\n",
    "from mnist_model import Net, train, test, device, optimizer_scheduler_generator, trainer, loaders\n",
    "\n",
    "# Load pretrained model\n",
    "model = torch.load(\"mnist_cnn.pt\")\n",
    "model.eval()\n",
    "\n",
    "# show the model stbructure, note that pruner will wrap the model layer.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4435816",
   "metadata": {},
   "source": [
    "### Performance and statistics of pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65e18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9919/10000 (99.19%)\n",
      "\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "| Index | Name  |  Type  |  Weight Shape  |    Input Size   |   Output Size   |  FLOPs   | #Params |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "|   0   | conv1 | Conv2d | (32, 1, 3, 3)  |  (3, 1, 28, 28) | (3, 32, 26, 26) |  194688  |   320   |\n",
      "|   1   | conv2 | Conv2d | (64, 32, 3, 3) | (3, 32, 26, 26) | (3, 64, 24, 24) | 10616832 |  18496  |\n",
      "|   2   | fc1   | Linear |  (128, 9216)   |    (3, 9216)    |     (3, 128)    | 1179648  | 1179776 |\n",
      "|   3   | fc2   | Linear |   (10, 128)    |     (3, 128)    |     (3, 10)     |   1280   |   1290  |\n",
      "+-------+-------+--------+----------------+-----------------+-----------------+----------+---------+\n",
      "FLOPs total: 11992448\n",
      "#Params total: 1199882\n",
      "Pretrained model FLOPs 11.99 M, #Params: 1.20M, Accuracy:  99.19%, Test-time:  1.4850s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pre_best_acc = test(model, device)\n",
    "pre_test_time = time.time() - start\n",
    "\n",
    "pre_flops, pre_params, _ = count_flops_params(model, torch.randn([3, 1, 28, 28]).to(device))\n",
    "print(f'Pretrained model FLOPs {pre_flops/1e6:.2f} M, #Params: {pre_params/1e6:.2f}M, Accuracy: {pre_best_acc: .2f}%, Test-time: {pre_test_time: .4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba20bd",
   "metadata": {},
   "source": [
    "# Pruning Model  Activation Mean Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ea7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.pytorch.pruning import ADMMPruner\n",
    "from nni.compression.pytorch.pruning import ActivationMeanRankPruner\n",
    "from nni.compression.pytorch.speedup import ModelSpeedup\n",
    "import nni\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pruner_function(config_list):\n",
    "\n",
    "    model = torch.load(\"mnist_cnn.pt\")\n",
    "    model.eval()\n",
    "\n",
    "    traced_optimizer = nni.trace(optim.Adadelta)(model.parameters(), lr=1.0)\n",
    "    criterion = F.nll_loss\n",
    "    \n",
    "    # Using ADMMPruner to prune the model and generate the masks.\n",
    "    #pruner = ADMMPruner(model, config_list, trainer, traced_optimizer, criterion, iterations=5, training_epochs=1, granularity='coarse-grained')\n",
    "    \n",
    "    pruner = ActivationMeanRankPruner(model, config_list, trainer, traced_optimizer, criterion, training_batches=20)\n",
    "    \n",
    "    # show the wrapped model structure, `PrunerModuleWrapper` have wrapped the layers that configured in the config_list.\n",
    "    #print(model)\n",
    "\n",
    "    # compress the model and generate the masks\n",
    "    _, masks = pruner.compress()\n",
    "\n",
    "    # show the masks sparsity\n",
    "    print(\"Showing the masks sparsity\")\n",
    "    for name, mask in masks.items():\n",
    "        print(name, ' sparsity : ', '{:.2}'.format(mask['weight'].sum() / mask['weight'].numel()))\n",
    "\n",
    "\n",
    "    # need to unwrap the model, if the model is wrapped before speedup\n",
    "    pruner._unwrap_model()\n",
    "\n",
    "    # speedup the model, for more information about speedup, please refer :doc:`pruning_speedup`.\n",
    "    ModelSpeedup(model, torch.rand(3, 1, 28, 28).to(device), masks).speedup_model()\n",
    "\n",
    "    #print(\"Model after speedup\")\n",
    "    #print(model)\n",
    "\n",
    "    optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "    \n",
    "    # fine- tuning model compacted model\n",
    "    # tuning and evaluate the model on MNIST dataset\n",
    "    total_epoch = 3\n",
    "    \n",
    "    for epoch in range(1, total_epoch + 1):\n",
    "        train(model, device, optimizer=optimizer, epoch=epoch)\n",
    "        test(model, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bb83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perfomance_function(model):\n",
    "    print(\"Model after speedup\")\n",
    "    print(model)\n",
    "    \n",
    "    start = time.time()\n",
    "    best_acc = test(model, device)\n",
    "    test_time = time.time() - start\n",
    "\n",
    "    flops, params, _ = count_flops_params(model, torch.randn([3, 1, 28, 28]).to(device))\n",
    "\n",
    "    print(f'Pretrained model FLOPs {pre_flops/1e6:.2f} M, #Params: {pre_params/1e6:.2f}M, Accuracy: {pre_best_acc: .2f}%, , Test-time: {pre_test_time: .4f}s')\n",
    "    print(f'Finetuned model FLOPs {flops/1e6:.2f} M, #Params: {params/1e6:.2f}M, Accuracy: {best_acc: .2f}%, Test-time: {test_time: .4f}s, Speed-up: {pre_test_time/test_time: .2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a1820",
   "metadata": {},
   "source": [
    "## ADMM Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10be2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-20 13:14:41] \u001b[33mWARNING: The old API trainer,traced_optimizer,criterion,training_batches,activation,mode,dummy_input will be deprecated after NNI v3.0, please using the new one evaluator,training_steps,activation,mode,dummy_input\u001b[0m\n",
      "[2022-11-20 13:14:41] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "[2022-11-20 13:14:41] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "[2022-11-20 13:14:41] \u001b[33mWARNING: op_names ['Linear'] not found in model\u001b[0m\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001160\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.349021\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.076578\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.009604\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.008620\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.225320\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.177449\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.000831\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.089915\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.030283\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.061408\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.011798\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.066102\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.012305\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.029739\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.033886\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.017012\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.022541\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.041949\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.007884\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.023137\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.066971\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.004157\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.027374\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.038471\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.035200\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.072131\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.045916\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.004398\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.120832\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.044706\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.066717\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.184067\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.012194\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.010522\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.011332\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.018899\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.035598\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.107398\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.002206\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.031931\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.028240\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.050689\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.017305\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.060027\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.000148\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.102866\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.116365\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.097281\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.027479\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.086985\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.056868\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.076208\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.140034\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.044764\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.078450\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.093771\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.116466\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.085080\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.159362\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.038103\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.021592\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.056483\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.006340\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.001376\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.010535\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.018455\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.014590\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.066555\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.108661\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.003422\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.046714\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.131448\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.045694\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.073738\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.092685\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.035328\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.063583\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.063789\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.077846\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.068256\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.009168\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.136024\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.002936\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.215785\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.183446\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.015311\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.027679\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.010637\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.110987\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.071902\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.078989\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.010623\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.004284\n",
      "Showing the masks sparsity\n",
      "conv1  sparsity :  0.16\n",
      "conv2  sparsity :  0.16\n",
      "[2022-11-20 13:14:56] \u001b[32mstart to speedup the model\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32minfer module masks...\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for conv1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::relu.6\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for conv2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::relu.7\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::max_pool2d.8\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for dropout1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::flatten.9\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for fc1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::relu.10\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for dropout2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for fc2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate mask for .aten::log_softmax.11\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::log_softmax.11\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the fc2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the dropout2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.10\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the fc1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::flatten.9\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the dropout1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::max_pool2d.8\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.7\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the conv2\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the .aten::relu.6\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mUpdate the indirect sparsity for the conv1\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mresolve the mask conflict\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace compressed modules...\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: conv1, op_type: Conv2d)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: conv2, op_type: Conv2d)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: dropout1, op_type: Dropout)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: fc1, op_type: Linear)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace linear with new in_features: 1440, out_features: 128\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: dropout2, op_type: Dropout)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace module (name: fc2, op_type: Linear)\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mreplace linear with new in_features: 128, out_features: 10\u001b[0m\n",
      "[2022-11-20 13:14:56] \u001b[32mspeedup done\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakaria/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.352154\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.059308\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.598679\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.339093\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.343754\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.345003\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.173841\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.193512\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.181260\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.174625\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.230777\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.310895\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.369451\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.276093\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.275271\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.263973\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.141542\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.145894\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.175095\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.185513\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.106667\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.095438\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.059577\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.149690\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.270041\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.206824\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.048990\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.222387\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.084822\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.069847\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.028515\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.165735\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.111469\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.094322\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.140563\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.222652\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.066389\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.088963\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.095071\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.144576\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.202840\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.063300\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.110979\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.085006\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.347273\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.119561\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.186556\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.080051\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.158222\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.022666\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.181283\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.054045\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.058493\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.065639\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.140464\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.188136\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.052685\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.324420\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.092550\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.146936\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.030890\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.149887\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.140373\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.211094\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.088128\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.100945\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.108559\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.108875\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.298710\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.145736\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.156615\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.124705\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.087688\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.076741\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.131283\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.100054\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.160309\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.036314\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.196372\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.076376\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.269866\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.154333\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.024835\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.068975\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.099426\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.044906\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.400426\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.221712\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.024886\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.154769\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.060070\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.114569\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.138105\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.088728\n",
      "\n",
      "Test set: Average loss: 0.0596, Accuracy: 9792/10000 (97.92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.060883\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.056297\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.338546\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.027255\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.038968\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.196884\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.046191\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.043158\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.119495\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.374387\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.094686\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.048337\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.055174\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.033931\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.010902\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.052807\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.014440\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.025188\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.039226\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.145594\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.063673\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.031124\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.127477\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.029091\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.101555\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.059026\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.095752\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.169497\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.079895\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.102269\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.035640\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.192329\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.079182\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.098249\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.076383\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.071350\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.088618\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.012431\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.025314\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.231633\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.036500\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.157239\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.046407\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.128567\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.209782\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.080644\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.128441\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.117426\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.037468\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.030724\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.089911\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.122655\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.020500\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.040523\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.018322\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.098913\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.086246\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.124702\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.213359\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.056543\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.102222\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.037770\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.077675\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.046976\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.135108\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.025915\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.034324\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.053342\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.031228\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.051337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.022449\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.203162\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.142555\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.115491\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.181027\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.008002\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.046301\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.099556\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.198656\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.068751\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.065964\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.029555\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.237303\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.083226\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.152990\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.164969\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.073163\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.035310\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.012144\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.243212\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.016072\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.234092\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.009925\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.031635\n",
      "\n",
      "Test set: Average loss: 0.0488, Accuracy: 9844/10000 (98.44%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.070085\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.149219\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.062270\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.024640\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.289713\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.013056\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.186942\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.084320\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.176366\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.022455\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019885\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.212829\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.063147\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.063455\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.034513\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.110444\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.033094\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.193605\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.108295\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.001846\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.036392\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.010461\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.083945\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.068780\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.025457\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.021695\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.073859\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.045150\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.019451\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.021884\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.278840\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.086585\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.091752\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.112578\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.042201\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.080460\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.074097\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.067548\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.109677\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.038805\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.003561\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.018975\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.026989\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.092208\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.118038\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.086602\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.130978\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.082831\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.049324\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.025264\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.115698\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.040651\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.071981\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.046246\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.067213\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.217392\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.062539\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.115906\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.056915\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.061427\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.030899\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.213461\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.092956\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.245480\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.050052\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.240094\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.092707\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.069401\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.080521\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.044418\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.016779\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.005051\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.198217\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.075088\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.068308\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.085084\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.215647\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.047363\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.077989\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.064328\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.038240\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.039732\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.074455\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.053362\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.037716\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.165164\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.051893\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.005526\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.228522\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.016580\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.080131\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.018033\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.008642\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.044413\n",
      "\n",
      "Test set: Average loss: 0.0391, Accuracy: 9871/10000 (98.71%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_list = [{\n",
    "    'op_types': ['Conv2d'],\n",
    "    'total_sparsity': 0.85\n",
    "    }, {\n",
    "    'op_names': ['Linear'],\n",
    "    'total_sparsity': 0.85\n",
    "    },\n",
    "    {\n",
    "    'exclude': True,\n",
    "    'op_names': ['fc2']\n",
    "}]\n",
    "\n",
    "\n",
    "pruned_model = pruner_function(config_list=config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a900dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after speedup\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1440, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 0.0391, Accuracy: 9871/10000 (98.71%)\n",
      "\n",
      "+-------+-------+--------+---------------+----------------+-----------------+--------+---------+\n",
      "| Index | Name  |  Type  |  Weight Shape |   Input Size   |   Output Size   | FLOPs  | #Params |\n",
      "+-------+-------+--------+---------------+----------------+-----------------+--------+---------+\n",
      "|   0   | conv1 | Conv2d |  (5, 1, 3, 3) | (3, 1, 28, 28) |  (3, 5, 26, 26) | 30420  |    50   |\n",
      "|   1   | conv2 | Conv2d | (10, 5, 3, 3) | (3, 5, 26, 26) | (3, 10, 24, 24) | 259200 |   460   |\n",
      "|   2   | fc1   | Linear |  (128, 1440)  |   (3, 1440)    |     (3, 128)    | 184320 |  184448 |\n",
      "|   3   | fc2   | Linear |   (10, 128)   |    (3, 128)    |     (3, 10)     |  1280  |   1290  |\n",
      "+-------+-------+--------+---------------+----------------+-----------------+--------+---------+\n",
      "FLOPs total: 475220\n",
      "#Params total: 186248\n",
      "Pretrained model FLOPs 11.99 M, #Params: 1.20M, Accuracy:  99.19%, , Test-time:  1.4850s\n",
      "Finetuned model FLOPs 0.48 M, #Params: 0.19M, Accuracy:  98.71%, Test-time:  1.5796s, Speed-up:  0.94x\n"
     ]
    }
   ],
   "source": [
    "Perfomance_function(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e4baa",
   "metadata": {},
   "source": [
    "## Knowledge Distillation on NNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddec9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c5ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillKL(nn.Module):\n",
    "    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n",
    "    def __init__(self, T):\n",
    "        super(DistillKL, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, y_s, y_t):\n",
    "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
    "        p_t = F.softmax(y_t/self.T, dim=1)\n",
    "        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]\n",
    "        return loss\n",
    "    \n",
    "def get_dummy_input(device):\n",
    "    dummy_input = torch.randn([3, 1, 28, 28]).to(device)\n",
    "    return dummy_input\n",
    "\n",
    "def get_model_optimizer_scheduler(model_t, model_s):\n",
    "    module_list = nn.ModuleList([])\n",
    "    module_list.append(model_s)\n",
    "    module_list.append(model_t)\n",
    "\n",
    "    # setup opotimizer for fine-tuning studeng model\n",
    "    #optimizer = torch.optim.SGD(model_s.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    #scheduler = MultiStepLR(optimizer, milestones=[int(160*0.5), int(160*0.75)], gamma=0.1)\n",
    "    optimizer, scheduler = optimizer_scheduler_generator(model_s)\n",
    "    \n",
    "    return module_list, optimizer, scheduler\n",
    "\n",
    "\n",
    "\n",
    "def train(models, device, train_loader, criterion, optimizer, epoch):\n",
    "    model_s = models[0].train()\n",
    "    model_t = models[-1].eval()\n",
    "    cri_cls = criterion\n",
    "    cri_kd = DistillKL(4)  # (args.kd_T)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_s = model_s(data)\n",
    "        output_t = model_t(data)\n",
    "\n",
    "        loss_cls = cri_cls(output_s, target)\n",
    "        loss_kd = cri_kd(output_s, output_t)\n",
    "        loss = loss_cls + loss_kd\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('Test Loss: {}  Accuracy: {}%\\n'.format(\n",
    "        test_loss, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624e6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = loaders()\n",
    "criterion = F.nll_loss\n",
    "models, optimizer, scheduler = get_model_optimizer_scheduler(model, pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85869717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.9109499566257e-05  Accuracy: 98.71%\n",
      "\n",
      "98.71\n",
      "Test Loss: 2.6686942018568516e-05  Accuracy: 99.19%\n",
      "\n",
      "99.19\n"
     ]
    }
   ],
   "source": [
    "## Test models\n",
    "print(test(models[0], device, criterion, test_loader))\n",
    "print(test(models[1], device, criterion, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c633d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fine-tuning...\n",
      "# Epoch 0 #\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.424398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakaria/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.666028\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.274616\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.337962\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.446332\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.242626\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.985083\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.363682\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.202556\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.020310\n",
      "Test Loss: 5.233486592769623e-05  Accuracy: 98.56%\n",
      "\n",
      "# Epoch 1 #\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.213060\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.919928\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.869347\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.026554\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.451874\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.257893\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.747673\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.817873\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.870108\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.866986\n",
      "Test Loss: 4.277927167713642e-05  Accuracy: 98.77%\n",
      "\n",
      "# Epoch 2 #\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.014212\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.870713\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.919065\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.147438\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.368107\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.029867\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.005896\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.865662\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.144033\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.654884\n",
      "Test Loss: 3.803734350949526e-05  Accuracy: 98.91%\n",
      "\n",
      "# Epoch 3 #\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.829066\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.931119\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.610785\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.162038\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.079507\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.987350\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.744109\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.882900\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.155001\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.217634\n",
      "Test Loss: 3.740927577018738e-05  Accuracy: 98.92%\n",
      "\n",
      "# Epoch 4 #\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.840201\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.825076\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.998314\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.884507\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.190152\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.745128\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.323444\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.784300\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.820863\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.866928\n",
      "Test Loss: 3.66814985871315e-05  Accuracy: 98.84%\n",
      "\n",
      "# Epoch 5 #\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.040254\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.142239\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.855044\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.836330\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.848808\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.952756\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.858081\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.749036\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.681585\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.183650\n",
      "Test Loss: 3.575578182935715e-05  Accuracy: 98.91%\n",
      "\n",
      "# Epoch 6 #\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.903157\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.781787\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.829171\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.964633\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.813793\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.176993\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.734037\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.784226\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.051039\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.552134\n",
      "Test Loss: 3.677289746701717e-05  Accuracy: 98.87%\n",
      "\n",
      "# Epoch 7 #\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.801285\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.866319\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.671133\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.524198\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.786844\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.667474\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.592981\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.080497\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.755505\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.247099\n",
      "Test Loss: 3.691333644092083e-05  Accuracy: 98.87%\n",
      "\n",
      "# Epoch 8 #\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.111917\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.633653\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.052554\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.895105\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.814508\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.736490\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.803613\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.098780\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.697591\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.628766\n",
      "Test Loss: 3.656118661165237e-05  Accuracy: 98.87%\n",
      "\n",
      "# Epoch 9 #\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.633619\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.382745\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.906397\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.698186\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.238563\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.885829\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.703127\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.080699\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.637745\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.689943\n",
      "Test Loss: 3.576974701136351e-05  Accuracy: 98.89%\n",
      "\n",
      "# Epoch 10 #\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.821975\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.819444\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.653169\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.859842\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.000661\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.751862\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.889476\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.254129\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.915843\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.781640\n",
      "Test Loss: 3.5620678775012496e-05  Accuracy: 98.9%\n",
      "\n",
      "# Epoch 11 #\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.821181\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.154878\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.859416\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.816988\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.853634\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.956501\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.885542\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.884003\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.790518\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.154094\n",
      "Test Loss: 3.639510851353407e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 12 #\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.994828\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.064110\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.980942\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.770538\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.848439\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.948871\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.035136\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.846426\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.122466\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.763771\n",
      "Test Loss: 3.564784917980433e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 13 #\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.054224\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 1.184001\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.874147\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.792648\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.659073\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.413092\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.759990\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.891108\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.789851\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.883261\n",
      "Test Loss: 3.5611553769558667e-05  Accuracy: 98.89%\n",
      "\n",
      "# Epoch 14 #\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.752874\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.948951\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.912336\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.826784\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.714650\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.775275\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.839527\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.716269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.067099\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.678898\n",
      "Test Loss: 3.578967303037643e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 15 #\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.590587\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 1.118263\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.640227\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.947409\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.077732\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.785652\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.576172\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.989677\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.923744\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.894200\n",
      "Test Loss: 3.5690776258707045e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 16 #\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.790208\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.995352\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.661511\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.844196\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.678627\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.761736\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.796104\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.776279\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.670426\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.837270\n",
      "Test Loss: 3.560706460848451e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 17 #\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.248687\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.984030\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.784652\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 1.043270\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.641577\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.981675\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.698707\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.720449\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.739876\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 1.096196\n",
      "Test Loss: 3.5614733211696146e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 18 #\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.817027\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.995767\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.738326\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.758833\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.062861\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.729217\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.904534\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.888715\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.941859\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.685001\n",
      "Test Loss: 3.556640092283487e-05  Accuracy: 98.88%\n",
      "\n",
      "# Epoch 19 #\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.708857\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.713012\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.795015\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.633699\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.928336\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.747231\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.530968\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 1.231594\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.838356\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.639540\n",
      "Test Loss: 3.560533616691828e-05  Accuracy: 98.89%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_top1 = 0\n",
    "fine_tune_epochs = 20 \n",
    "\n",
    "print('start fine-tuning...')\n",
    "\n",
    "for epoch in range(fine_tune_epochs):\n",
    "    print('# Epoch {} #'.format(epoch))\n",
    "    train(models, device, train_loader, criterion, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # test student only\n",
    "    top1 = test(models[0], device, criterion, test_loader)\n",
    "    if top1 > best_top1:\n",
    "        best_top1 = top1\n",
    "        updated_model = deepcopy(models[0])\n",
    "        torch.save(updated_model, \"mnist_prunned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef397d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.740927744656801e-05  Accuracy: 98.92%\n",
      "\n",
      "98.92\n"
     ]
    }
   ],
   "source": [
    "print(test(updated_model, device, criterion, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c79d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(updated_model.state_dict(), 'model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e7961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = torch.jit.script(updated_model)\n",
    "torch.jit.save(compiled_model, 'updated_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
